"""Code generated by Speakeasy (https://speakeasy.com). DO NOT EDIT."""

from .basesdk import BaseSDK
from sudo_ai import errors, models, utils
from sudo_ai._hooks import HookContext
from sudo_ai.types import OptionalNullable, UNSET
from sudo_ai.utils import eventstreaming, get_security_from_env
from sudo_ai.utils.unmarshal_json_response import unmarshal_json_response
from typing import Any, List, Mapping, Optional, Union


class Responses(BaseSDK):
    def create_response(
        self,
        *,
        background: OptionalNullable[bool] = UNSET,
        conversation: OptionalNullable[
            Union[
                models.ResponsesRequestConversation,
                models.ResponsesRequestConversationTypedDict,
            ]
        ] = UNSET,
        include: OptionalNullable[List[Any]] = UNSET,
        input_: OptionalNullable[
            Union[models.ResponsesRequestInput, models.ResponsesRequestInputTypedDict]
        ] = UNSET,
        instructions: OptionalNullable[str] = UNSET,
        max_output_tokens: OptionalNullable[int] = UNSET,
        max_tool_calls: OptionalNullable[int] = UNSET,
        metadata: Optional[Any] = None,
        model: OptionalNullable[str] = UNSET,
        parallel_tool_calls: OptionalNullable[bool] = UNSET,
        previous_response_id: OptionalNullable[str] = UNSET,
        prompt: Optional[Any] = None,
        prompt_cache_key: OptionalNullable[str] = UNSET,
        reasoning: Optional[Any] = None,
        safety_identifier: OptionalNullable[str] = UNSET,
        service_tier: OptionalNullable[str] = UNSET,
        store: OptionalNullable[bool] = UNSET,
        stream: Optional[bool] = False,
        stream_options: Optional[Any] = None,
        temperature: OptionalNullable[float] = UNSET,
        text: Optional[Any] = None,
        tool_choice: OptionalNullable[
            Union[
                models.ResponsesRequestToolChoice,
                models.ResponsesRequestToolChoiceTypedDict,
            ]
        ] = UNSET,
        tools: OptionalNullable[List[Any]] = UNSET,
        top_logprobs: OptionalNullable[int] = UNSET,
        top_p: OptionalNullable[float] = UNSET,
        truncation: OptionalNullable[str] = UNSET,
        retries: OptionalNullable[utils.RetryConfig] = UNSET,
        server_url: Optional[str] = None,
        timeout_ms: Optional[int] = None,
        http_headers: Optional[Mapping[str, str]] = None,
    ) -> models.Response:
        r"""*[OpenAI Only]* Responses API: Create a model response for the given input

        :param background:
        :param conversation:
        :param include:
        :param input:
        :param instructions:
        :param max_output_tokens:
        :param max_tool_calls:
        :param metadata:
        :param model:
        :param parallel_tool_calls:
        :param previous_response_id:
        :param prompt:
        :param prompt_cache_key:
        :param reasoning:
        :param safety_identifier:
        :param service_tier:
        :param store:
        :param stream: If set, partial message deltas and streaming events will be sent. For regular HTTP responses, this must be false.
        :param stream_options:
        :param temperature:
        :param text:
        :param tool_choice:
        :param tools:
        :param top_logprobs:
        :param top_p:
        :param truncation:
        :param retries: Override the default retry configuration for this method
        :param server_url: Override the default server URL for this method
        :param timeout_ms: Override the default request timeout configuration for this method in milliseconds
        :param http_headers: Additional headers to set or replace on requests.
        """
        base_url = None
        url_variables = None
        if timeout_ms is None:
            timeout_ms = self.sdk_configuration.timeout_ms

        if server_url is not None:
            base_url = server_url
        else:
            base_url = self._get_url(base_url, url_variables)

        request = models.ResponsesRequest(
            background=background,
            conversation=conversation,
            include=include,
            input=utils.get_pydantic_model(
                input_, OptionalNullable[models.ResponsesRequestInput]
            ),
            instructions=instructions,
            max_output_tokens=max_output_tokens,
            max_tool_calls=max_tool_calls,
            metadata=metadata,
            model=model,
            parallel_tool_calls=parallel_tool_calls,
            previous_response_id=previous_response_id,
            prompt=prompt,
            prompt_cache_key=prompt_cache_key,
            reasoning=reasoning,
            safety_identifier=safety_identifier,
            service_tier=service_tier,
            store=store,
            stream=stream,
            stream_options=stream_options,
            temperature=temperature,
            text=text,
            tool_choice=tool_choice,
            tools=tools,
            top_logprobs=top_logprobs,
            top_p=top_p,
            truncation=truncation,
        )

        req = self._build_request(
            method="POST",
            path="/v1/responses",
            base_url=base_url,
            url_variables=url_variables,
            request=request,
            request_body_required=True,
            request_has_path_params=False,
            request_has_query_params=True,
            user_agent_header="user-agent",
            accept_header_value="application/json",
            http_headers=http_headers,
            security=self.sdk_configuration.security,
            get_serialized_body=lambda: utils.serialize_request_body(
                request, False, False, "json", models.ResponsesRequest
            ),
            timeout_ms=timeout_ms,
        )

        if retries == UNSET:
            if self.sdk_configuration.retry_config is not UNSET:
                retries = self.sdk_configuration.retry_config

        retry_config = None
        if isinstance(retries, utils.RetryConfig):
            retry_config = (retries, ["429", "500", "502", "503", "504"])

        http_res = self.do_request(
            hook_ctx=HookContext(
                config=self.sdk_configuration,
                base_url=base_url or "",
                operation_id="createResponse",
                oauth2_scopes=None,
                security_source=get_security_from_env(
                    self.sdk_configuration.security, models.Security
                ),
            ),
            request=req,
            error_status_codes=["400", "401", "4XX", "500", "502", "5XX"],
            retry_config=retry_config,
        )

        response_data: Any = None
        if utils.match_response(http_res, "200", "application/json"):
            return unmarshal_json_response(models.Response, http_res)
        if utils.match_response(http_res, ["400", "401"], "application/json"):
            response_data = unmarshal_json_response(errors.ErrorResponseData, http_res)
            raise errors.ErrorResponse(response_data, http_res)
        if utils.match_response(http_res, ["500", "502"], "application/json"):
            response_data = unmarshal_json_response(errors.ErrorResponseData, http_res)
            raise errors.ErrorResponse(response_data, http_res)
        if utils.match_response(http_res, "4XX", "*"):
            http_res_text = utils.stream_to_text(http_res)
            raise errors.SudoDefaultError("API error occurred", http_res, http_res_text)
        if utils.match_response(http_res, "5XX", "*"):
            http_res_text = utils.stream_to_text(http_res)
            raise errors.SudoDefaultError("API error occurred", http_res, http_res_text)

        raise errors.SudoDefaultError("Unexpected response received", http_res)

    async def create_response_async(
        self,
        *,
        background: OptionalNullable[bool] = UNSET,
        conversation: OptionalNullable[
            Union[
                models.ResponsesRequestConversation,
                models.ResponsesRequestConversationTypedDict,
            ]
        ] = UNSET,
        include: OptionalNullable[List[Any]] = UNSET,
        input_: OptionalNullable[
            Union[models.ResponsesRequestInput, models.ResponsesRequestInputTypedDict]
        ] = UNSET,
        instructions: OptionalNullable[str] = UNSET,
        max_output_tokens: OptionalNullable[int] = UNSET,
        max_tool_calls: OptionalNullable[int] = UNSET,
        metadata: Optional[Any] = None,
        model: OptionalNullable[str] = UNSET,
        parallel_tool_calls: OptionalNullable[bool] = UNSET,
        previous_response_id: OptionalNullable[str] = UNSET,
        prompt: Optional[Any] = None,
        prompt_cache_key: OptionalNullable[str] = UNSET,
        reasoning: Optional[Any] = None,
        safety_identifier: OptionalNullable[str] = UNSET,
        service_tier: OptionalNullable[str] = UNSET,
        store: OptionalNullable[bool] = UNSET,
        stream: Optional[bool] = False,
        stream_options: Optional[Any] = None,
        temperature: OptionalNullable[float] = UNSET,
        text: Optional[Any] = None,
        tool_choice: OptionalNullable[
            Union[
                models.ResponsesRequestToolChoice,
                models.ResponsesRequestToolChoiceTypedDict,
            ]
        ] = UNSET,
        tools: OptionalNullable[List[Any]] = UNSET,
        top_logprobs: OptionalNullable[int] = UNSET,
        top_p: OptionalNullable[float] = UNSET,
        truncation: OptionalNullable[str] = UNSET,
        retries: OptionalNullable[utils.RetryConfig] = UNSET,
        server_url: Optional[str] = None,
        timeout_ms: Optional[int] = None,
        http_headers: Optional[Mapping[str, str]] = None,
    ) -> models.Response:
        r"""*[OpenAI Only]* Responses API: Create a model response for the given input

        :param background:
        :param conversation:
        :param include:
        :param input:
        :param instructions:
        :param max_output_tokens:
        :param max_tool_calls:
        :param metadata:
        :param model:
        :param parallel_tool_calls:
        :param previous_response_id:
        :param prompt:
        :param prompt_cache_key:
        :param reasoning:
        :param safety_identifier:
        :param service_tier:
        :param store:
        :param stream: If set, partial message deltas and streaming events will be sent. For regular HTTP responses, this must be false.
        :param stream_options:
        :param temperature:
        :param text:
        :param tool_choice:
        :param tools:
        :param top_logprobs:
        :param top_p:
        :param truncation:
        :param retries: Override the default retry configuration for this method
        :param server_url: Override the default server URL for this method
        :param timeout_ms: Override the default request timeout configuration for this method in milliseconds
        :param http_headers: Additional headers to set or replace on requests.
        """
        base_url = None
        url_variables = None
        if timeout_ms is None:
            timeout_ms = self.sdk_configuration.timeout_ms

        if server_url is not None:
            base_url = server_url
        else:
            base_url = self._get_url(base_url, url_variables)

        request = models.ResponsesRequest(
            background=background,
            conversation=conversation,
            include=include,
            input=utils.get_pydantic_model(
                input_, OptionalNullable[models.ResponsesRequestInput]
            ),
            instructions=instructions,
            max_output_tokens=max_output_tokens,
            max_tool_calls=max_tool_calls,
            metadata=metadata,
            model=model,
            parallel_tool_calls=parallel_tool_calls,
            previous_response_id=previous_response_id,
            prompt=prompt,
            prompt_cache_key=prompt_cache_key,
            reasoning=reasoning,
            safety_identifier=safety_identifier,
            service_tier=service_tier,
            store=store,
            stream=stream,
            stream_options=stream_options,
            temperature=temperature,
            text=text,
            tool_choice=tool_choice,
            tools=tools,
            top_logprobs=top_logprobs,
            top_p=top_p,
            truncation=truncation,
        )

        req = self._build_request_async(
            method="POST",
            path="/v1/responses",
            base_url=base_url,
            url_variables=url_variables,
            request=request,
            request_body_required=True,
            request_has_path_params=False,
            request_has_query_params=True,
            user_agent_header="user-agent",
            accept_header_value="application/json",
            http_headers=http_headers,
            security=self.sdk_configuration.security,
            get_serialized_body=lambda: utils.serialize_request_body(
                request, False, False, "json", models.ResponsesRequest
            ),
            timeout_ms=timeout_ms,
        )

        if retries == UNSET:
            if self.sdk_configuration.retry_config is not UNSET:
                retries = self.sdk_configuration.retry_config

        retry_config = None
        if isinstance(retries, utils.RetryConfig):
            retry_config = (retries, ["429", "500", "502", "503", "504"])

        http_res = await self.do_request_async(
            hook_ctx=HookContext(
                config=self.sdk_configuration,
                base_url=base_url or "",
                operation_id="createResponse",
                oauth2_scopes=None,
                security_source=get_security_from_env(
                    self.sdk_configuration.security, models.Security
                ),
            ),
            request=req,
            error_status_codes=["400", "401", "4XX", "500", "502", "5XX"],
            retry_config=retry_config,
        )

        response_data: Any = None
        if utils.match_response(http_res, "200", "application/json"):
            return unmarshal_json_response(models.Response, http_res)
        if utils.match_response(http_res, ["400", "401"], "application/json"):
            response_data = unmarshal_json_response(errors.ErrorResponseData, http_res)
            raise errors.ErrorResponse(response_data, http_res)
        if utils.match_response(http_res, ["500", "502"], "application/json"):
            response_data = unmarshal_json_response(errors.ErrorResponseData, http_res)
            raise errors.ErrorResponse(response_data, http_res)
        if utils.match_response(http_res, "4XX", "*"):
            http_res_text = await utils.stream_to_text_async(http_res)
            raise errors.SudoDefaultError("API error occurred", http_res, http_res_text)
        if utils.match_response(http_res, "5XX", "*"):
            http_res_text = await utils.stream_to_text_async(http_res)
            raise errors.SudoDefaultError("API error occurred", http_res, http_res_text)

        raise errors.SudoDefaultError("Unexpected response received", http_res)

    def create_streaming_response(
        self,
        *,
        background: OptionalNullable[bool] = UNSET,
        conversation: OptionalNullable[
            Union[
                models.ResponsesRequestStreamConversation,
                models.ResponsesRequestStreamConversationTypedDict,
            ]
        ] = UNSET,
        include: OptionalNullable[List[Any]] = UNSET,
        input_: OptionalNullable[
            Union[
                models.ResponsesRequestStreamInput,
                models.ResponsesRequestStreamInputTypedDict,
            ]
        ] = UNSET,
        instructions: OptionalNullable[str] = UNSET,
        max_output_tokens: OptionalNullable[int] = UNSET,
        max_tool_calls: OptionalNullable[int] = UNSET,
        metadata: Optional[Any] = None,
        model: OptionalNullable[str] = UNSET,
        parallel_tool_calls: OptionalNullable[bool] = UNSET,
        previous_response_id: OptionalNullable[str] = UNSET,
        prompt: Optional[Any] = None,
        prompt_cache_key: OptionalNullable[str] = UNSET,
        reasoning: Optional[Any] = None,
        safety_identifier: OptionalNullable[str] = UNSET,
        service_tier: OptionalNullable[str] = UNSET,
        store: OptionalNullable[bool] = UNSET,
        stream: Optional[bool] = True,
        stream_options: Optional[Any] = None,
        temperature: OptionalNullable[float] = UNSET,
        text: Optional[Any] = None,
        tool_choice: OptionalNullable[
            Union[
                models.ResponsesRequestStreamToolChoice,
                models.ResponsesRequestStreamToolChoiceTypedDict,
            ]
        ] = UNSET,
        tools: OptionalNullable[List[Any]] = UNSET,
        top_logprobs: OptionalNullable[int] = UNSET,
        top_p: OptionalNullable[float] = UNSET,
        truncation: OptionalNullable[str] = UNSET,
        retries: OptionalNullable[utils.RetryConfig] = UNSET,
        server_url: Optional[str] = None,
        timeout_ms: Optional[int] = None,
        http_headers: Optional[Mapping[str, str]] = None,
    ) -> eventstreaming.EventStream[models.ResponseEvent]:
        r"""*[OpenAI Only]* Responses API: Create a streaming model response for the given input using server-sent events.

        :param background:
        :param conversation:
        :param include:
        :param input:
        :param instructions:
        :param max_output_tokens:
        :param max_tool_calls:
        :param metadata:
        :param model:
        :param parallel_tool_calls:
        :param previous_response_id:
        :param prompt:
        :param prompt_cache_key:
        :param reasoning:
        :param safety_identifier:
        :param service_tier:
        :param store:
        :param stream: If set, partial message deltas and streaming events will be sent. For streaming responses, this must be set to true.
        :param stream_options:
        :param temperature:
        :param text:
        :param tool_choice:
        :param tools:
        :param top_logprobs:
        :param top_p:
        :param truncation:
        :param retries: Override the default retry configuration for this method
        :param server_url: Override the default server URL for this method
        :param timeout_ms: Override the default request timeout configuration for this method in milliseconds
        :param http_headers: Additional headers to set or replace on requests.
        """
        base_url = None
        url_variables = None
        if timeout_ms is None:
            timeout_ms = self.sdk_configuration.timeout_ms

        if server_url is not None:
            base_url = server_url
        else:
            base_url = self._get_url(base_url, url_variables)

        request = models.ResponsesRequestStream(
            background=background,
            conversation=conversation,
            include=include,
            input=utils.get_pydantic_model(
                input_, OptionalNullable[models.ResponsesRequestStreamInput]
            ),
            instructions=instructions,
            max_output_tokens=max_output_tokens,
            max_tool_calls=max_tool_calls,
            metadata=metadata,
            model=model,
            parallel_tool_calls=parallel_tool_calls,
            previous_response_id=previous_response_id,
            prompt=prompt,
            prompt_cache_key=prompt_cache_key,
            reasoning=reasoning,
            safety_identifier=safety_identifier,
            service_tier=service_tier,
            store=store,
            stream=stream,
            stream_options=stream_options,
            temperature=temperature,
            text=text,
            tool_choice=tool_choice,
            tools=tools,
            top_logprobs=top_logprobs,
            top_p=top_p,
            truncation=truncation,
        )

        req = self._build_request(
            method="POST",
            path="/v1/responses#stream",
            base_url=base_url,
            url_variables=url_variables,
            request=request,
            request_body_required=True,
            request_has_path_params=False,
            request_has_query_params=True,
            user_agent_header="user-agent",
            accept_header_value="text/event-stream",
            http_headers=http_headers,
            security=self.sdk_configuration.security,
            get_serialized_body=lambda: utils.serialize_request_body(
                request, False, False, "json", models.ResponsesRequestStream
            ),
            timeout_ms=timeout_ms,
        )

        if retries == UNSET:
            if self.sdk_configuration.retry_config is not UNSET:
                retries = self.sdk_configuration.retry_config

        retry_config = None
        if isinstance(retries, utils.RetryConfig):
            retry_config = (retries, ["429", "500", "502", "503", "504"])

        http_res = self.do_request(
            hook_ctx=HookContext(
                config=self.sdk_configuration,
                base_url=base_url or "",
                operation_id="createStreamingResponse",
                oauth2_scopes=None,
                security_source=get_security_from_env(
                    self.sdk_configuration.security, models.Security
                ),
            ),
            request=req,
            error_status_codes=["400", "401", "4XX", "500", "502", "5XX"],
            stream=True,
            retry_config=retry_config,
        )

        response_data: Any = None
        if utils.match_response(http_res, "200", "text/event-stream"):
            return eventstreaming.EventStream(
                http_res,
                lambda raw: utils.unmarshal_json(raw, models.ResponseEvent),
                sentinel="[DONE]",
                client_ref=self,
            )
        if utils.match_response(http_res, ["400", "401"], "application/json"):
            http_res_text = utils.stream_to_text(http_res)
            response_data = unmarshal_json_response(
                errors.ErrorResponseData, http_res, http_res_text
            )
            raise errors.ErrorResponse(response_data, http_res, http_res_text)
        if utils.match_response(http_res, ["500", "502"], "application/json"):
            http_res_text = utils.stream_to_text(http_res)
            response_data = unmarshal_json_response(
                errors.ErrorResponseData, http_res, http_res_text
            )
            raise errors.ErrorResponse(response_data, http_res, http_res_text)
        if utils.match_response(http_res, "4XX", "*"):
            http_res_text = utils.stream_to_text(http_res)
            raise errors.SudoDefaultError("API error occurred", http_res, http_res_text)
        if utils.match_response(http_res, "5XX", "*"):
            http_res_text = utils.stream_to_text(http_res)
            raise errors.SudoDefaultError("API error occurred", http_res, http_res_text)

        http_res_text = utils.stream_to_text(http_res)
        raise errors.SudoDefaultError(
            "Unexpected response received", http_res, http_res_text
        )

    async def create_streaming_response_async(
        self,
        *,
        background: OptionalNullable[bool] = UNSET,
        conversation: OptionalNullable[
            Union[
                models.ResponsesRequestStreamConversation,
                models.ResponsesRequestStreamConversationTypedDict,
            ]
        ] = UNSET,
        include: OptionalNullable[List[Any]] = UNSET,
        input_: OptionalNullable[
            Union[
                models.ResponsesRequestStreamInput,
                models.ResponsesRequestStreamInputTypedDict,
            ]
        ] = UNSET,
        instructions: OptionalNullable[str] = UNSET,
        max_output_tokens: OptionalNullable[int] = UNSET,
        max_tool_calls: OptionalNullable[int] = UNSET,
        metadata: Optional[Any] = None,
        model: OptionalNullable[str] = UNSET,
        parallel_tool_calls: OptionalNullable[bool] = UNSET,
        previous_response_id: OptionalNullable[str] = UNSET,
        prompt: Optional[Any] = None,
        prompt_cache_key: OptionalNullable[str] = UNSET,
        reasoning: Optional[Any] = None,
        safety_identifier: OptionalNullable[str] = UNSET,
        service_tier: OptionalNullable[str] = UNSET,
        store: OptionalNullable[bool] = UNSET,
        stream: Optional[bool] = True,
        stream_options: Optional[Any] = None,
        temperature: OptionalNullable[float] = UNSET,
        text: Optional[Any] = None,
        tool_choice: OptionalNullable[
            Union[
                models.ResponsesRequestStreamToolChoice,
                models.ResponsesRequestStreamToolChoiceTypedDict,
            ]
        ] = UNSET,
        tools: OptionalNullable[List[Any]] = UNSET,
        top_logprobs: OptionalNullable[int] = UNSET,
        top_p: OptionalNullable[float] = UNSET,
        truncation: OptionalNullable[str] = UNSET,
        retries: OptionalNullable[utils.RetryConfig] = UNSET,
        server_url: Optional[str] = None,
        timeout_ms: Optional[int] = None,
        http_headers: Optional[Mapping[str, str]] = None,
    ) -> eventstreaming.EventStreamAsync[models.ResponseEvent]:
        r"""*[OpenAI Only]* Responses API: Create a streaming model response for the given input using server-sent events.

        :param background:
        :param conversation:
        :param include:
        :param input:
        :param instructions:
        :param max_output_tokens:
        :param max_tool_calls:
        :param metadata:
        :param model:
        :param parallel_tool_calls:
        :param previous_response_id:
        :param prompt:
        :param prompt_cache_key:
        :param reasoning:
        :param safety_identifier:
        :param service_tier:
        :param store:
        :param stream: If set, partial message deltas and streaming events will be sent. For streaming responses, this must be set to true.
        :param stream_options:
        :param temperature:
        :param text:
        :param tool_choice:
        :param tools:
        :param top_logprobs:
        :param top_p:
        :param truncation:
        :param retries: Override the default retry configuration for this method
        :param server_url: Override the default server URL for this method
        :param timeout_ms: Override the default request timeout configuration for this method in milliseconds
        :param http_headers: Additional headers to set or replace on requests.
        """
        base_url = None
        url_variables = None
        if timeout_ms is None:
            timeout_ms = self.sdk_configuration.timeout_ms

        if server_url is not None:
            base_url = server_url
        else:
            base_url = self._get_url(base_url, url_variables)

        request = models.ResponsesRequestStream(
            background=background,
            conversation=conversation,
            include=include,
            input=utils.get_pydantic_model(
                input_, OptionalNullable[models.ResponsesRequestStreamInput]
            ),
            instructions=instructions,
            max_output_tokens=max_output_tokens,
            max_tool_calls=max_tool_calls,
            metadata=metadata,
            model=model,
            parallel_tool_calls=parallel_tool_calls,
            previous_response_id=previous_response_id,
            prompt=prompt,
            prompt_cache_key=prompt_cache_key,
            reasoning=reasoning,
            safety_identifier=safety_identifier,
            service_tier=service_tier,
            store=store,
            stream=stream,
            stream_options=stream_options,
            temperature=temperature,
            text=text,
            tool_choice=tool_choice,
            tools=tools,
            top_logprobs=top_logprobs,
            top_p=top_p,
            truncation=truncation,
        )

        req = self._build_request_async(
            method="POST",
            path="/v1/responses#stream",
            base_url=base_url,
            url_variables=url_variables,
            request=request,
            request_body_required=True,
            request_has_path_params=False,
            request_has_query_params=True,
            user_agent_header="user-agent",
            accept_header_value="text/event-stream",
            http_headers=http_headers,
            security=self.sdk_configuration.security,
            get_serialized_body=lambda: utils.serialize_request_body(
                request, False, False, "json", models.ResponsesRequestStream
            ),
            timeout_ms=timeout_ms,
        )

        if retries == UNSET:
            if self.sdk_configuration.retry_config is not UNSET:
                retries = self.sdk_configuration.retry_config

        retry_config = None
        if isinstance(retries, utils.RetryConfig):
            retry_config = (retries, ["429", "500", "502", "503", "504"])

        http_res = await self.do_request_async(
            hook_ctx=HookContext(
                config=self.sdk_configuration,
                base_url=base_url or "",
                operation_id="createStreamingResponse",
                oauth2_scopes=None,
                security_source=get_security_from_env(
                    self.sdk_configuration.security, models.Security
                ),
            ),
            request=req,
            error_status_codes=["400", "401", "4XX", "500", "502", "5XX"],
            stream=True,
            retry_config=retry_config,
        )

        response_data: Any = None
        if utils.match_response(http_res, "200", "text/event-stream"):
            return eventstreaming.EventStreamAsync(
                http_res,
                lambda raw: utils.unmarshal_json(raw, models.ResponseEvent),
                sentinel="[DONE]",
                client_ref=self,
            )
        if utils.match_response(http_res, ["400", "401"], "application/json"):
            http_res_text = await utils.stream_to_text_async(http_res)
            response_data = unmarshal_json_response(
                errors.ErrorResponseData, http_res, http_res_text
            )
            raise errors.ErrorResponse(response_data, http_res, http_res_text)
        if utils.match_response(http_res, ["500", "502"], "application/json"):
            http_res_text = await utils.stream_to_text_async(http_res)
            response_data = unmarshal_json_response(
                errors.ErrorResponseData, http_res, http_res_text
            )
            raise errors.ErrorResponse(response_data, http_res, http_res_text)
        if utils.match_response(http_res, "4XX", "*"):
            http_res_text = await utils.stream_to_text_async(http_res)
            raise errors.SudoDefaultError("API error occurred", http_res, http_res_text)
        if utils.match_response(http_res, "5XX", "*"):
            http_res_text = await utils.stream_to_text_async(http_res)
            raise errors.SudoDefaultError("API error occurred", http_res, http_res_text)

        http_res_text = await utils.stream_to_text_async(http_res)
        raise errors.SudoDefaultError(
            "Unexpected response received", http_res, http_res_text
        )
