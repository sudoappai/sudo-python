"""Code generated by Speakeasy (https://speakeasy.com). DO NOT EDIT."""

from __future__ import annotations
from .item_union import ItemUnion, ItemUnionTypedDict
from pydantic import model_serializer
from sudo_ai.types import BaseModel, Nullable, OptionalNullable, UNSET, UNSET_SENTINEL
from typing import Any, List, Optional, Union
from typing_extensions import NotRequired, TypeAliasType, TypedDict


class ResponseConversationTypedDict(TypedDict):
    r"""The ID of the conversation this response belongs to"""

    id: str


class ResponseConversation(BaseModel):
    r"""The ID of the conversation this response belongs to"""

    id: str


class IncompleteDetailsTypedDict(TypedDict):
    r"""Details about why the response is incomplete (if status is \"incomplete\")"""

    reason: str
    r"""The reason the response is incomplete"""


class IncompleteDetails(BaseModel):
    r"""Details about why the response is incomplete (if status is \"incomplete\")"""

    reason: str
    r"""The reason the response is incomplete"""


InstructionsTypedDict = TypeAliasType("InstructionsTypedDict", Union[str, List[Any]])
r"""Instructions provided to the model"""


Instructions = TypeAliasType("Instructions", Union[str, List[Any]])
r"""Instructions provided to the model"""


ResponseToolChoiceTypedDict = TypeAliasType(
    "ResponseToolChoiceTypedDict", Union[str, Any]
)
r"""Tool choice configuration"""


ResponseToolChoice = TypeAliasType("ResponseToolChoice", Union[str, Any])
r"""Tool choice configuration"""


class ResponseUsageTypedDict(TypedDict):
    r"""Usage statistics for the API call"""

    input_tokens: int
    r"""Number of tokens in the input"""
    output_tokens: int
    r"""Number of tokens in the output"""
    total_tokens: int
    r"""Total number of tokens used"""
    input_tokens_details: NotRequired[Any]
    r"""Breakdown of input token details"""
    output_tokens_details: NotRequired[Any]
    r"""Breakdown of output token details"""


class ResponseUsage(BaseModel):
    r"""Usage statistics for the API call"""

    input_tokens: int
    r"""Number of tokens in the input"""

    output_tokens: int
    r"""Number of tokens in the output"""

    total_tokens: int
    r"""Total number of tokens used"""

    input_tokens_details: Optional[Any] = None
    r"""Breakdown of input token details"""

    output_tokens_details: Optional[Any] = None
    r"""Breakdown of output token details"""


class ResponseTypedDict(TypedDict):
    created_at: int
    r"""Unix timestamp (in seconds) when the response was created"""
    id: str
    r"""Unique identifier for the response"""
    model: str
    r"""The model used to generate the response"""
    object: str
    r"""The object type, always \"response\" """
    status: str
    r"""The status of the response (e.g., \"in_progress\", \"completed\", \"failed\", \"cancelled\", \"incomplete\")"""
    background: NotRequired[Nullable[bool]]
    r"""Whether the response was generated in the background"""
    conversation: NotRequired[Nullable[ResponseConversationTypedDict]]
    r"""The ID of the conversation this response belongs to"""
    error: NotRequired[Any]
    r"""Error information if the response failed"""
    incomplete_details: NotRequired[Nullable[IncompleteDetailsTypedDict]]
    r"""Details about why the response is incomplete (if status is \"incomplete\")"""
    instructions: NotRequired[Nullable[InstructionsTypedDict]]
    r"""Instructions provided to the model"""
    max_output_tokens: NotRequired[Nullable[int]]
    r"""Maximum number of output tokens"""
    max_tool_calls: NotRequired[Nullable[int]]
    r"""Maximum number of tool calls"""
    metadata: NotRequired[Any]
    r"""Set of key-value pairs that can be attached to an object"""
    output: NotRequired[Nullable[List[ItemUnionTypedDict]]]
    r"""The output items generated by the model"""
    parallel_tool_calls: NotRequired[Nullable[bool]]
    r"""Whether parallel tool calls are enabled"""
    previous_response_id: NotRequired[Nullable[str]]
    r"""ID of the previous response"""
    prompt: NotRequired[Any]
    r"""Prompt configuration"""
    prompt_cache_key: NotRequired[Nullable[str]]
    r"""Prompt cache key"""
    reasoning: NotRequired[Any]
    r"""Reasoning configuration"""
    safety_identifier: NotRequired[Nullable[str]]
    r"""Safety identifier"""
    service_tier: NotRequired[Nullable[str]]
    r"""Service tier used"""
    temperature: NotRequired[Nullable[float]]
    r"""Temperature sampling parameter"""
    text: NotRequired[Any]
    r"""Text configuration"""
    tool_choice: NotRequired[Nullable[ResponseToolChoiceTypedDict]]
    r"""Tool choice configuration"""
    tools: NotRequired[Nullable[List[Any]]]
    r"""Tools available to the model"""
    top_logprobs: NotRequired[Nullable[int]]
    r"""Number of top log probabilities to return"""
    top_p: NotRequired[Nullable[float]]
    r"""Top-p sampling parameter"""
    truncation: NotRequired[Nullable[str]]
    r"""Truncation strategy"""
    usage: NotRequired[Nullable[ResponseUsageTypedDict]]
    r"""Usage statistics for the API call"""


class Response(BaseModel):
    created_at: int
    r"""Unix timestamp (in seconds) when the response was created"""

    id: str
    r"""Unique identifier for the response"""

    model: str
    r"""The model used to generate the response"""

    object: str
    r"""The object type, always \"response\" """

    status: str
    r"""The status of the response (e.g., \"in_progress\", \"completed\", \"failed\", \"cancelled\", \"incomplete\")"""

    background: OptionalNullable[bool] = UNSET
    r"""Whether the response was generated in the background"""

    conversation: OptionalNullable[ResponseConversation] = UNSET
    r"""The ID of the conversation this response belongs to"""

    error: Optional[Any] = None
    r"""Error information if the response failed"""

    incomplete_details: OptionalNullable[IncompleteDetails] = UNSET
    r"""Details about why the response is incomplete (if status is \"incomplete\")"""

    instructions: OptionalNullable[Instructions] = UNSET
    r"""Instructions provided to the model"""

    max_output_tokens: OptionalNullable[int] = UNSET
    r"""Maximum number of output tokens"""

    max_tool_calls: OptionalNullable[int] = UNSET
    r"""Maximum number of tool calls"""

    metadata: Optional[Any] = None
    r"""Set of key-value pairs that can be attached to an object"""

    output: OptionalNullable[List[ItemUnion]] = UNSET
    r"""The output items generated by the model"""

    parallel_tool_calls: OptionalNullable[bool] = UNSET
    r"""Whether parallel tool calls are enabled"""

    previous_response_id: OptionalNullable[str] = UNSET
    r"""ID of the previous response"""

    prompt: Optional[Any] = None
    r"""Prompt configuration"""

    prompt_cache_key: OptionalNullable[str] = UNSET
    r"""Prompt cache key"""

    reasoning: Optional[Any] = None
    r"""Reasoning configuration"""

    safety_identifier: OptionalNullable[str] = UNSET
    r"""Safety identifier"""

    service_tier: OptionalNullable[str] = UNSET
    r"""Service tier used"""

    temperature: OptionalNullable[float] = UNSET
    r"""Temperature sampling parameter"""

    text: Optional[Any] = None
    r"""Text configuration"""

    tool_choice: OptionalNullable[ResponseToolChoice] = UNSET
    r"""Tool choice configuration"""

    tools: OptionalNullable[List[Any]] = UNSET
    r"""Tools available to the model"""

    top_logprobs: OptionalNullable[int] = UNSET
    r"""Number of top log probabilities to return"""

    top_p: OptionalNullable[float] = UNSET
    r"""Top-p sampling parameter"""

    truncation: OptionalNullable[str] = UNSET
    r"""Truncation strategy"""

    usage: OptionalNullable[ResponseUsage] = UNSET
    r"""Usage statistics for the API call"""

    @model_serializer(mode="wrap")
    def serialize_model(self, handler):
        optional_fields = [
            "background",
            "conversation",
            "error",
            "incomplete_details",
            "instructions",
            "max_output_tokens",
            "max_tool_calls",
            "metadata",
            "output",
            "parallel_tool_calls",
            "previous_response_id",
            "prompt",
            "prompt_cache_key",
            "reasoning",
            "safety_identifier",
            "service_tier",
            "temperature",
            "text",
            "tool_choice",
            "tools",
            "top_logprobs",
            "top_p",
            "truncation",
            "usage",
        ]
        nullable_fields = [
            "background",
            "conversation",
            "incomplete_details",
            "instructions",
            "max_output_tokens",
            "max_tool_calls",
            "output",
            "parallel_tool_calls",
            "previous_response_id",
            "prompt_cache_key",
            "safety_identifier",
            "service_tier",
            "temperature",
            "tool_choice",
            "tools",
            "top_logprobs",
            "top_p",
            "truncation",
            "usage",
        ]
        null_default_fields = []

        serialized = handler(self)

        m = {}

        for n, f in type(self).model_fields.items():
            k = f.alias or n
            val = serialized.get(k)
            serialized.pop(k, None)

            optional_nullable = k in optional_fields and k in nullable_fields
            is_set = (
                self.__pydantic_fields_set__.intersection({n})
                or k in null_default_fields
            )  # pylint: disable=no-member

            if val is not None and val != UNSET_SENTINEL:
                m[k] = val
            elif val != UNSET_SENTINEL and (
                not k in optional_fields or (optional_nullable and is_set)
            ):
                m[k] = val

        return m
