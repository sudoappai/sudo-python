"""Code generated by Speakeasy (https://speakeasy.com). DO NOT EDIT."""

from __future__ import annotations
from .inputitem_union import InputItemUnion, InputItemUnionTypedDict
from pydantic import model_serializer
from sudo_ai.types import BaseModel, Nullable, OptionalNullable, UNSET, UNSET_SENTINEL
from typing import Any, List, Optional, Union
from typing_extensions import NotRequired, TypeAliasType, TypedDict


ResponsesRequestConversationTypedDict = TypeAliasType(
    "ResponsesRequestConversationTypedDict", Union[str, Any]
)


ResponsesRequestConversation = TypeAliasType(
    "ResponsesRequestConversation", Union[str, Any]
)


ResponsesRequestInputTypedDict = TypeAliasType(
    "ResponsesRequestInputTypedDict", Union[str, List[InputItemUnionTypedDict]]
)


ResponsesRequestInput = TypeAliasType(
    "ResponsesRequestInput", Union[str, List[InputItemUnion]]
)


ResponsesRequestToolChoiceTypedDict = TypeAliasType(
    "ResponsesRequestToolChoiceTypedDict", Union[str, Any]
)


ResponsesRequestToolChoice = TypeAliasType(
    "ResponsesRequestToolChoice", Union[str, Any]
)


class ResponsesRequestTypedDict(TypedDict):
    background: NotRequired[Nullable[bool]]
    conversation: NotRequired[Nullable[ResponsesRequestConversationTypedDict]]
    include: NotRequired[Nullable[List[Any]]]
    input: NotRequired[Nullable[ResponsesRequestInputTypedDict]]
    instructions: NotRequired[Nullable[str]]
    max_output_tokens: NotRequired[Nullable[int]]
    max_tool_calls: NotRequired[Nullable[int]]
    metadata: NotRequired[Any]
    model: NotRequired[Nullable[str]]
    parallel_tool_calls: NotRequired[Nullable[bool]]
    previous_response_id: NotRequired[Nullable[str]]
    prompt: NotRequired[Any]
    prompt_cache_key: NotRequired[Nullable[str]]
    reasoning: NotRequired[Any]
    safety_identifier: NotRequired[Nullable[str]]
    service_tier: NotRequired[Nullable[str]]
    store: NotRequired[Nullable[bool]]
    stream: NotRequired[bool]
    r"""If set, partial message deltas and streaming events will be sent. For regular HTTP responses, this must be false."""
    stream_options: NotRequired[Any]
    temperature: NotRequired[Nullable[float]]
    text: NotRequired[Any]
    tool_choice: NotRequired[Nullable[ResponsesRequestToolChoiceTypedDict]]
    tools: NotRequired[Nullable[List[Any]]]
    top_logprobs: NotRequired[Nullable[int]]
    top_p: NotRequired[Nullable[float]]
    truncation: NotRequired[Nullable[str]]


class ResponsesRequest(BaseModel):
    background: OptionalNullable[bool] = UNSET

    conversation: OptionalNullable[ResponsesRequestConversation] = UNSET

    include: OptionalNullable[List[Any]] = UNSET

    input: OptionalNullable[ResponsesRequestInput] = UNSET

    instructions: OptionalNullable[str] = UNSET

    max_output_tokens: OptionalNullable[int] = UNSET

    max_tool_calls: OptionalNullable[int] = UNSET

    metadata: Optional[Any] = None

    model: OptionalNullable[str] = UNSET

    parallel_tool_calls: OptionalNullable[bool] = UNSET

    previous_response_id: OptionalNullable[str] = UNSET

    prompt: Optional[Any] = None

    prompt_cache_key: OptionalNullable[str] = UNSET

    reasoning: Optional[Any] = None

    safety_identifier: OptionalNullable[str] = UNSET

    service_tier: OptionalNullable[str] = UNSET

    store: OptionalNullable[bool] = UNSET

    stream: Optional[bool] = False
    r"""If set, partial message deltas and streaming events will be sent. For regular HTTP responses, this must be false."""

    stream_options: Optional[Any] = None

    temperature: OptionalNullable[float] = UNSET

    text: Optional[Any] = None

    tool_choice: OptionalNullable[ResponsesRequestToolChoice] = UNSET

    tools: OptionalNullable[List[Any]] = UNSET

    top_logprobs: OptionalNullable[int] = UNSET

    top_p: OptionalNullable[float] = UNSET

    truncation: OptionalNullable[str] = UNSET

    @model_serializer(mode="wrap")
    def serialize_model(self, handler):
        optional_fields = [
            "background",
            "conversation",
            "include",
            "input",
            "instructions",
            "max_output_tokens",
            "max_tool_calls",
            "metadata",
            "model",
            "parallel_tool_calls",
            "previous_response_id",
            "prompt",
            "prompt_cache_key",
            "reasoning",
            "safety_identifier",
            "service_tier",
            "store",
            "stream",
            "stream_options",
            "temperature",
            "text",
            "tool_choice",
            "tools",
            "top_logprobs",
            "top_p",
            "truncation",
        ]
        nullable_fields = [
            "background",
            "conversation",
            "include",
            "input",
            "instructions",
            "max_output_tokens",
            "max_tool_calls",
            "model",
            "parallel_tool_calls",
            "previous_response_id",
            "prompt_cache_key",
            "safety_identifier",
            "service_tier",
            "store",
            "temperature",
            "tool_choice",
            "tools",
            "top_logprobs",
            "top_p",
            "truncation",
        ]
        null_default_fields = []

        serialized = handler(self)

        m = {}

        for n, f in type(self).model_fields.items():
            k = f.alias or n
            val = serialized.get(k)
            serialized.pop(k, None)

            optional_nullable = k in optional_fields and k in nullable_fields
            is_set = (
                self.__pydantic_fields_set__.intersection({n})
                or k in null_default_fields
            )  # pylint: disable=no-member

            if val is not None and val != UNSET_SENTINEL:
                m[k] = val
            elif val != UNSET_SENTINEL and (
                not k in optional_fields or (optional_nullable and is_set)
            ):
                m[k] = val

        return m
